{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoforum term extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests as req\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "from parts import collect, preprocessing, oie, domain_relevance\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"https://www.motor-talk.de/forum/audi-80-90-100-200-v8-b158.html\"\n",
    "\n",
    "car_links = collect.get_links_car(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicht relevant und bricht programm \n",
    "#car_links.remove(\"https://www.motor-talk.de/forum/faq-linksammlung-audi-80-90-100-t593977.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_car_domain(links):\n",
    "    domain = []\n",
    "    \n",
    "    for link in tqdm(links):\n",
    "        car_text = \" ; \".join(collect.get_text_car(\"https://www.motor-talk.de\"+link))\n",
    "        if len(car_text) < 100000 and car_text:\n",
    "            domain.append(car_text)\n",
    "    \n",
    "    return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#car_domain = get_car_domain(car_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "### export car_domain to files\n",
    "path = \"resources/car_domain/\"\n",
    "counter = 0\n",
    "\n",
    "for post in car_domain:\n",
    "    with open(path+str(counter)+\".txt\", \"w\") as file:\n",
    "        file.write(post)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import car_domain from files\n",
    "path = \"resources/car_domain/\"\n",
    "counter = 10\n",
    "car_domain = []\n",
    "\n",
    "while counter:\n",
    "    try:\n",
    "        with open(path+str(counter)+\".txt\", \"r\") as file:\n",
    "            car_domain.append(file.read())\n",
    "        counter -= 1\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oie(corpus):\n",
    "    # decision logic for extracting roots and terms - for better analysis sentences are passed as well\n",
    "\n",
    "    roots = []\n",
    "    terms = []\n",
    "    sents = []\n",
    "\n",
    "    doc = nlp(corpus.lower())\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        t = set()\n",
    "        # get sentences\n",
    "        sents.append(sent.text)\n",
    "\n",
    "        # get important tokens from sentence\n",
    "        pd, oc, ng = \"\", \"\", \"\"\n",
    "        for token in sent:\n",
    "            if token.dep_ == \"pd\":\n",
    "                pd = token.lemma_\n",
    "            if token.dep_ == \"oc\":\n",
    "                oc = token.lemma_\n",
    "            if token.dep_ == \"ng\" and token.head.dep_ == \"ROOT\":\n",
    "                ng = token.lemma_\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                t.add(token.text)\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                t.add(token.text)\n",
    "\n",
    "        for chunk in sent.noun_chunks:\n",
    "            c = []\n",
    "            for token in chunk:\n",
    "                if not token.is_stop and not token.pos_ == \"DET\":\n",
    "                    c.append(token.text)\n",
    "            if len(c)>1:\n",
    "                t.add(\" \".join(c))\n",
    "\n",
    "        # get roots / predicate depending on sentence structure\n",
    "        r = []\n",
    "        if ng:\n",
    "            r.append(ng)\n",
    "        if sent.root.pos_ == \"AUX\" and pd:\n",
    "            r.append(pd)\n",
    "        if sent.root.pos_ == \"AUX\" and oc:\n",
    "            r.append(oc)\n",
    "        else:\n",
    "            r.append(sent.root.lemma_)\n",
    "\n",
    "        roots.append(' '.join(r))\n",
    "        terms.append(t)\n",
    "\n",
    "    return roots, terms, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:00<00:00, 31.46it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:00<00:00, 12.14it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.96it/s][A\n"
     ]
    }
   ],
   "source": [
    "test_terms = []\n",
    "\n",
    "for doc in tqdm(car_domain):\n",
    "    doc_terms = []\n",
    "    roots, terms, sents = get_oie(doc)\n",
    "    for t in terms:\n",
    "        for term in t:\n",
    "            doc_terms.append(term)\n",
    "    \n",
    "    test_terms.append(doc_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:00<00:00, 37.84it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:00<00:00, 26.99it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:00<00:00, 11.64it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.18it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "### create label list of terms\n",
    "\n",
    "roots_tolabel = []\n",
    "terms_tolabel = []\n",
    "sents_tolabel = []\n",
    "\n",
    "for doc in tqdm(car_domain):\n",
    "    roots, terms, sents = get_oie(doc)\n",
    "    for r, t, s in zip(roots, terms, sents):\n",
    "        for term in t:\n",
    "            roots_tolabel.append(r)\n",
    "            terms_tolabel.append(term)\n",
    "            sents_tolabel.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### export label list\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(roots_tolabel, columns=[\"roots\"])\n",
    "df[\"terms\"] = pd.DataFrame(terms_tolabel)\n",
    "df[\"sents\"] = pd.DataFrame(sents_tolabel)\n",
    "\n",
    "df.reset_index().to_csv(\"terms_tolabel.csv\", index = False, header=True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_tf(terms):\n",
    "    flat_terms = [item for sublist in terms for item in sublist]\n",
    "    tf = Counter(flat_terms)\n",
    "    max_freq = Counter(flat_terms).most_common(1)[0][1]\n",
    "    for t in tf:\n",
    "        tf[t] = (tf[t]/max_freq)\n",
    "    \n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_idf(terms):\n",
    "    flat_terms = [item for sublist in terms for item in set(sublist)]\n",
    "    idf = Counter(flat_terms)\n",
    "    for t in idf:\n",
    "        idf[t] = np.log2(len(terms)/idf[t])\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_tdf(terms):\n",
    "    flat_terms = [item for sublist in terms for item in set(sublist)]\n",
    "    tdf = Counter(flat_terms)\n",
    "    for t in tdf:\n",
    "        tdf[t] = tdf[t]/len(terms)\n",
    "    \n",
    "    return tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = get_tf(terms)\n",
    "idf = get_idf(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "\n",
    "for term in set([item for sublist in terms for item in sublist]):\n",
    "    tf_idf[term] = tf[term]*idf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.Series(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hallo       1.00\n",
       "motor       0.80\n",
       "audi        0.74\n",
       "zitat       0.65\n",
       "mfg         0.64\n",
       "getriebe    0.50\n",
       "problem     0.50\n",
       "auto        0.47\n",
       "stecker     0.38\n",
       "fragen      0.35\n",
       "probleme    0.32\n",
       "gruß        0.28\n",
       "danke       0.28\n",
       "b4          0.27\n",
       "mai         0.27\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().to_csv(\"car_terms.csv\", index = False, header=True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import term list to label to genereate prelabeled list\n",
    "\n",
    "counter = 0   \n",
    "reader = csv.reader(open('car_terms_labeled.csv', 'r'), delimiter=';')\n",
    "terms = []\n",
    "for row in reader:\n",
    "    _, k, v, _ = row\n",
    "    counter += 1\n",
    "    try:\n",
    "        labeled[k] = float(v)\n",
    "    except:\n",
    "        labeled[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def post_term_cleaning(terms, label):\n",
    "    # clean zitate, datum, zeichen/ zahlen only terms, zeit, links\n",
    "    clean_terms = []\n",
    "    labels = []\n",
    "    time, date, link, zitat, ireg, abbr = 0, 0, 0, 0, 0, 0\n",
    "    abbreviations = preprocessing.get_abbr()\n",
    "\n",
    "    for doc in terms:\n",
    "        clean_doc = []\n",
    "        doc_labels = {}\n",
    "        for term in doc:\n",
    "            doc_labels[term] = 0\n",
    "            if len(term) < 2:\n",
    "                pass\n",
    "            elif re.search(r\"\\d\\d:\\d\\d:\\d\\d\", term):\n",
    "                time += 1\n",
    "            elif re.search(r\"\\d\\d.\\d\\d.\\d\\d\\d\\d\", term):\n",
    "                date += 1\n",
    "            elif re.search(\"www\", term):\n",
    "                link += 1\n",
    "            elif re.search(\"@\", term):\n",
    "                zitat += 1\n",
    "            elif term in abbreviations:\n",
    "                abbr += 1\n",
    "            elif re.search(r\"\\w\", term):\n",
    "                clean_doc.append(term)\n",
    "                doc_labels[term] = 1\n",
    "            elif re.search(r\"^\\W\", term):\n",
    "                ireg += 1\n",
    "            else:\n",
    "                print(term)\n",
    "                clean_doc.append(term)\n",
    "                doc_labels[term] = 1\n",
    "        \n",
    "        clean_terms.append(clean_doc)\n",
    "        labels.append(doc_labels)\n",
    "        \n",
    "\n",
    "    print(\"deleted time references:\", time)\n",
    "    print(\"deleted date references:\", date)\n",
    "    print(\"deleted links:\", link)\n",
    "    print(\"deleted quotes:\", zitat)\n",
    "    print(\"deleted ireg expressions:\", ireg)\n",
    "    print(\"deleted abbreviations:\", abbr)\n",
    "    \n",
    "    if label:\n",
    "        return labels\n",
    "    else:\n",
    "        return clean_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted time references: 3\n",
      "deleted date references: 1\n",
      "deleted links: 6\n",
      "deleted quotes: 5\n",
      "deleted ireg expressions: 0\n",
      "deleted abbreviations: 9\n"
     ]
    }
   ],
   "source": [
    "clean_terms = post_term_cleaning(test_terms, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted time references: 3\n",
      "deleted date references: 1\n",
      "deleted links: 6\n",
      "deleted quotes: 5\n",
      "deleted ireg expressions: 0\n",
      "deleted abbreviations: 9\n"
     ]
    }
   ],
   "source": [
    "prelabeled_terms = post_term_cleaning(test_terms, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([item for sublist in prelabeled_terms for item in sublist])\n",
    "df[\"pred\"] = pd.DataFrame([sublist[item] for sublist in prelabeled_terms for item in sublist])\n",
    "\n",
    "df.to_csv(\"prelabeled_terms.csv\", index = False, header=True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = get_tf(clean_terms)\n",
    "idf = get_idf(clean_terms)\n",
    "tdf = get_tdf(clean_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "tf_tdf = {}\n",
    "\n",
    "for term in set([item for sublist in clean_terms for item in sublist]):\n",
    "    tf_idf[term] = tf[term]*idf[term]\n",
    "    tf_tdf[term] = tf[term]*tdf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.Series(tf_tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().to_csv(\"car_terms.csv\", index = False, header=True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted time references: 0\n",
      "deleted date references: 0\n",
      "deleted links: 0\n",
      "deleted quotes: 0\n",
      "deleted ireg expressions: 0\n",
      "deleted abbreviations: 0\n",
      "[[], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(post_term_cleaning([\"12.12.2020\",\"lol\",\"hallo\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
